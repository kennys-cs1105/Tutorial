# 动手学深度学习

*Created by KennyS*

---


## 微调

1. 数据集
    - Fashion-MNIST：6w+
    - ImageNet：1000w+, 1000classes

2. 假定我们拥有数据集数量介于两者之间
    - 适用于ImageNet的复杂模型可能会在我们的数据集上过拟合
    - 而我们的数据集数量有限，训练模型的准确性无法满足实际要求

3. 解决方案
    - 收集更多的数据，但成本高
    - 迁移学习：将源数据集学习到的知识迁移到目标数据集。尽管ImageNet数据集中的大多数图像与椅子无关，但在此数据集上训练的模型可能会提取更通用的图像特征，这有助于识别边缘、纹理、形状和对象组合。 这些类似的特征也可能有效地识别椅子。

### 步骤

1. 在源数据集（例如ImageNet数据集）上预训练神经网络模型，即源模型。

2. 创建一个新的神经网络模型，即目标模型。这将复制源模型上的所有模型设计及其参数（输出层除外）。我们假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集。我们还假设源模型的输出层与源数据集的标签密切相关；因此不在目标模型中使用该层。

3. 向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。

4. 在目标数据集（如椅子数据集）上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调。

5. 当目标数据集比源数据集小得多时，微调有助于提高模型的泛化能力。

### 示例

1. 基于ImageNet训练的ResNet-18微调自己的数据集
2. 特征层参数使用预训练权重，修改分类层（输出类别数）
3. 在目标数据集较小的情况下，使用小学习率微调

    ```python
    pretrained_net = torchvision.models.resnet18(pretrained=True)
    finetune_net = torchvision.models.resnet18(pretrained=True)
    finetune_net.fc = nn.Linear(finetune_net.fc.in_features, 2)
    nn.init.xavier_uniform_(finetune_net.fc.weight) # 初始化分类层的参数

    # 如果param_group=True，输出层中的模型参数将使用十倍的学习率
    def train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5,
                        param_group=True):
        train_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(
            os.path.join(data_dir, 'train'), transform=train_augs),
            batch_size=batch_size, shuffle=True)
        test_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(
            os.path.join(data_dir, 'test'), transform=test_augs),
            batch_size=batch_size)
        devices = d2l.try_all_gpus()
        loss = nn.CrossEntropyLoss(reduction="none")
        if param_group:
            params_1x = [param for name, param in net.named_parameters()
                if name not in ["fc.weight", "fc.bias"]]
            trainer = torch.optim.SGD([{'params': params_1x},
                                    {'params': net.fc.parameters(),
                                        'lr': learning_rate * 10}],
                                    lr=learning_rate, weight_decay=0.001)
        else:
            trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,
                                    weight_decay=0.001)
        d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
                    devices)

    train_fine_tuning(finetune_net, 5e-5)
    ```

---

## 目标检测

1. 得到目标类别 + 位置

### 边界框

1. 描述目标空间位置，矩形
    - 左上角和右下角的x，y
    - 边界框中心的(x, y)以及框的宽度、高度
    - 坐标系原点在左上角, x轴向右为正, y轴向下为正

    ```python
    #@save
    def box_corner_to_center(boxes):
        """从（左上，右下）转换到（中间，宽度，高度）"""
        x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
        cx = (x1 + x2) / 2
        cy = (y1 + y2) / 2
        w = x2 - x1
        h = y2 - y1
        boxes = torch.stack((cx, cy, w, h), axis=-1)
        return boxes

    #@save
    def box_center_to_corner(boxes):
        """从（中间，宽度，高度）转换到（左上，右下）"""
        cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
        x1 = cx - 0.5 * w
        y1 = cy - 0.5 * h
        x2 = cx + 0.5 * w
        y2 = cy + 0.5 * h
        boxes = torch.stack((x1, y1, x2, y2), axis=-1)
        return boxes

    #@save
    def bbox_to_rect(bbox, color):
        # 将边界框(左上x,左上y,右下x,右下y)格式转换成matplotlib格式：
        # ((左上x,左上y),宽,高)
        return d2l.plt.Rectangle(
            xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],
            fill=False, edgecolor=color, linewidth=2)
    ```

