# Tutorial for EasyRL

*Created By KennyS*

---

## 强化学习基础

### 序列决策

#### 智能体与环境

强化学习研究智能体与环境交互的问题。智能体的目的是从观测中学到能最大化奖励的策略。


#### 奖励

奖励是由环境给的一种标量的反馈信号。强化学习的目的就是最大化智能体可以获得的奖励。例如：
- 象棋选手，最后结束的时候正奖励是（win），负奖励是（lose）
- 股票管理中，奖励为盈利或损失

#### 序列决策

在与环境的交互过程中，智能体会获得许多观测。针对每一个观测，智能体会采取一个动作，也会得到一个奖励。所以历史是观测、动作、奖励的序列：

$H_{t} = o_{1}, a_{1}, r_{1}, ..., o_{t}, a_{t}, r_{t}$

智能体在采取当前动作的时候会依赖于他之前得到的历史，所以可以将整个游戏的状态看成这个历史的函数：

$S_{t} = f(H_{t})$

状态和观测的关系：在强化学习中，使用实值的向量、矩阵或更高阶的张量来表示状态或观测。例如，可以用RGB像素值的矩阵来表示一个视觉的观测，可以使用机器人关节的角度和速度来表示一个状态。

环境有自己的函数$s_{t}^{e} = f^{e}(H_{t})$来更新状态。在智能体的内部也有一个函数$s_{t}^{a} = f^{a}(H_{t})$来更新状态。当智能体的状态和环境的状态等价时，强化学习通常被建模成一个马尔可夫决策过程的问题。在马尔可夫决策过程中，$o_{t} = s_{t}^{e} = s_{t}^{a}$

但是有一种情况是智能体得到的观测并不能包含环境运作的所有状态。因为在强化学习的设定中，环境的状态才是真正的所有状态。这种情况称为部分可观测的。在这种情况下，强化学习通常被建模成一个部分可观测马尔可夫决策过程的问题。例如在自动驾驶中，智能体只能感知传感器采集的有限的环境信息。这个过程可以用一个七元组表示：$(S, A, T, R, \Omega, O, \gamma)$。其中，$S$表示状态空间，为隐变量，$A$为动作空间，$T( {s}' | s,a)$为状态转移概率，$R$为奖励函数，$\Omega(o|s,a)$为观测概率，$O$为观测空间，$\gamma$为折扣系数。


### 强化学习智能体的组成和类型

对于一个强化学习智能体，他可能有一个或多个如下的组成：
- 策略(policy): 智能体会用策略来选取下一步的动作
- 价值函数(value function): 智能体会用价值函数来评估当前状态或动作的价值
- 模型(model): 智能体会用模型来预测下一个状态和奖励


#### 策略

策略是智能体的动作模型，决定了智能体的动作。他其实是一个函数，把当前的状态作为输入，输出一个动作。

**随机性策略**：就是$\pi(a|s)$函数，他输出的是一个动作的概率分布。例如，在一个有四个动作的环境中，$\pi(a|s) = p(a_{t}=a | s_{t}=s)$。输入一个状态$s$，输出一个概率。这个概率是智能体所有动作的概率。然后对这个概率分布进行采样，可以得到智能体即将采取的动作。

**确定性策略**：就是$\pi(a|s) = \delta(a, \pi(s))$函数，他输出的是一个动作。例如，在一个有四个动作的环境中，$\pi(a|s)$可能输出$a_{2}$，表示在状态s下，动作a的概率为1。
