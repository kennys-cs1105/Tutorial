{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "1. greedy search -> beam search\n",
    "    - greedy search: 只选择top1 logit的token\n",
    "        - [batch_size, seq_len, inc]\n",
    "    - beam search：增加候选的数量, 束宽度 beam width\n",
    "        - [batch_size * num_beams, seq_len, inc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./assert/beam_search.PNG\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"./assert/beam_search.PNG\", width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法分析\n",
    "\n",
    "1. 传统算法选择单个节点概率最大的值, 但是同一条链上的多个节点的概率值熵却不一定是最大的, 这样有可能会错过最佳束\n",
    "2. beam search算法修正了这一点, 保证top K个高概率值, 就能得到实际更优的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34013722497f4eefb857cded44e6b019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a513806532441799870e52f72fab75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/367 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447bdab38bae47c7985275fc70a534d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16141366d3942849c9eea02b556ef1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a67f7611f3402299ef0a91662af352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd1827a4d1e438a985da3d9a439f921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e5b4913287471d98d02d3988cd676c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, it was said that the Lord had said to Moses, \"I will give\n",
      "Hi I am a big fan of this book. I have been reading it for a while now and\n"
     ]
    }
   ],
   "source": [
    "prefixes = [\"Once upon a time\", \"Hi I am a\"]\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "input_ids = tokenizer(prefixes, return_tensors=\"pt\").input_ids\n",
    "output_ids = model.generate(input_ids, num_beams=3, max_length=20)\n",
    "\n",
    "output_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "for text in output_text:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7454,  2402,   257,   640],\n",
       "        [17250,   314,   716,   257]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 7454,  2402,   257,   640,    11,   262,   995,   373,   257,  1295,\n",
       "           286,  1049,  8737,   290,  1049,  3514,    13,   383,   995,   373],\n",
       "        [17250,   314,   716,   257,  1263,  4336,   286,   262,   649,   366,\n",
       "            47,  9990, 32767,     1, 14256,    13,   314,   423,   587,   284]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output = model.generate(input_ids, max_length=20)\n",
    "greedy_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step by step\n",
    "\n",
    "- $logp_{1} + logp_{2} = log(p_{1} \\cdot p_{2})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_beam_search_steps(model, tokenizer, prefix, num_beams=3, max_steps=3):\n",
    "    # 将输入文本转换为 token ids\n",
    "    input_ids = tokenizer(prefix, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    # 初始化 beam 状态\n",
    "    current_beams = [(input_ids, 0)]  # (sequence, score)\n",
    "    \n",
    "    print(f\"\\n开始处理前缀: '{prefix}'\")\n",
    "    \n",
    "    # 对每一步进行 beam search\n",
    "    for step in range(max_steps):\n",
    "        candidates = []\n",
    "        print(f\"\\n第 {step + 1} 步:\")\n",
    "        \n",
    "        # 对每个当前的 beam 进行扩展\n",
    "        for beam_ids, beam_score in current_beams:\n",
    "            # 获取模型输出\n",
    "            with torch.no_grad():\n",
    "                outputs = model(beam_ids)\n",
    "                next_token_logits = outputs.logits[:, -1, :]\n",
    "                next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            # 获取前 num_beams 个最可能的下一个 token\n",
    "            values, indices = torch.topk(next_token_probs, num_beams)\n",
    "            \n",
    "            # 为每个可能的下一个 token 创建新的候选项\n",
    "            for value, index in zip(values[0], indices[0]):\n",
    "                new_ids = torch.cat([beam_ids, index.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "                new_score = beam_score + torch.log(value).item()\n",
    "                candidates.append((new_ids, new_score))\n",
    "                \n",
    "                # 打印当前候选项\n",
    "                new_text = tokenizer.decode(new_ids[0])\n",
    "                print(f\"候选项: {new_text}({new_ids[0].tolist()}) 分数: {new_score:.4f}\")\n",
    "        \n",
    "        # 选择前 num_beams 个最佳候选项\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        current_beams = candidates[:num_beams]\n",
    "        print(\"\\n选择的 beam:\")\n",
    "        for beam_ids, beam_score in current_beams:\n",
    "            print(f\"beam: {tokenizer.decode(beam_ids[0])}({beam_ids[0].tolist()}) 分数: {beam_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始处理前缀: 'Once upon a time'\n",
      "\n",
      "第 1 步:\n",
      "候选项: Once upon a time,([7454, 2402, 257, 640, 11]) 分数: -0.8512\n",
      "候选项: Once upon a time the([7454, 2402, 257, 640, 262]) 分数: -2.7396\n",
      "候选项: Once upon a time I([7454, 2402, 257, 640, 314]) 分数: -3.2029\n",
      "\n",
      "选择的 beam:\n",
      "beam: Once upon a time,([7454, 2402, 257, 640, 11]) 分数: -0.8512\n",
      "beam: Once upon a time the([7454, 2402, 257, 640, 262]) 分数: -2.7396\n",
      "beam: Once upon a time I([7454, 2402, 257, 640, 314]) 分数: -3.2029\n",
      "\n",
      "第 2 步:\n",
      "候选项: Once upon a time, the([7454, 2402, 257, 640, 11, 262]) 分数: -3.0523\n",
      "候选项: Once upon a time, I([7454, 2402, 257, 640, 11, 314]) 分数: -3.6055\n",
      "候选项: Once upon a time, it([7454, 2402, 257, 640, 11, 340]) 分数: -4.0718\n",
      "候选项: Once upon a time the world([7454, 2402, 257, 640, 262, 995]) 分数: -6.5612\n",
      "候选项: Once upon a time the sun([7454, 2402, 257, 640, 262, 4252]) 分数: -7.6559\n",
      "候选项: Once upon a time the people([7454, 2402, 257, 640, 262, 661]) 分数: -7.7589\n",
      "候选项: Once upon a time I was([7454, 2402, 257, 640, 314, 373]) 分数: -4.8047\n",
      "候选项: Once upon a time I had([7454, 2402, 257, 640, 314, 550]) 分数: -5.7435\n",
      "候选项: Once upon a time I thought([7454, 2402, 257, 640, 314, 1807]) 分数: -6.5308\n",
      "\n",
      "选择的 beam:\n",
      "beam: Once upon a time, the([7454, 2402, 257, 640, 11, 262]) 分数: -3.0523\n",
      "beam: Once upon a time, I([7454, 2402, 257, 640, 11, 314]) 分数: -3.6055\n",
      "beam: Once upon a time, it([7454, 2402, 257, 640, 11, 340]) 分数: -4.0718\n",
      "\n",
      "第 3 步:\n",
      "候选项: Once upon a time, the world([7454, 2402, 257, 640, 11, 262, 995]) 分数: -7.0757\n",
      "候选项: Once upon a time, the people([7454, 2402, 257, 640, 11, 262, 661]) 分数: -8.2539\n",
      "候选项: Once upon a time, the two([7454, 2402, 257, 640, 11, 262, 734]) 分数: -8.3031\n",
      "候选项: Once upon a time, I was([7454, 2402, 257, 640, 11, 314, 373]) 分数: -5.5660\n",
      "候选项: Once upon a time, I had([7454, 2402, 257, 640, 11, 314, 550]) 分数: -6.2779\n",
      "候选项: Once upon a time, I would([7454, 2402, 257, 640, 11, 314, 561]) 分数: -6.8436\n",
      "候选项: Once upon a time, it was([7454, 2402, 257, 640, 11, 340, 373]) 分数: -5.1921\n",
      "候选项: Once upon a time, it seemed([7454, 2402, 257, 640, 11, 340, 3947]) 分数: -6.7970\n",
      "候选项: Once upon a time, it would([7454, 2402, 257, 640, 11, 340, 561]) 分数: -6.8182\n",
      "\n",
      "选择的 beam:\n",
      "beam: Once upon a time, it was([7454, 2402, 257, 640, 11, 340, 373]) 分数: -5.1921\n",
      "beam: Once upon a time, I was([7454, 2402, 257, 640, 11, 314, 373]) 分数: -5.5660\n",
      "beam: Once upon a time, I had([7454, 2402, 257, 640, 11, 314, 550]) 分数: -6.2779\n"
     ]
    }
   ],
   "source": [
    "show_beam_search_steps(model, tokenizer, prefixes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解读\n",
    "\n",
    "1. 每个节点后面都有top3个候选, 输出的是概率相加过后的top3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始处理前缀: 'Hi I am a'\n",
      "\n",
      "第 1 步:\n",
      "候选项: Hi I am a big([17250, 314, 716, 257, 1263]) 分数: -3.8471\n",
      "候选项: Hi I am a very([17250, 314, 716, 257, 845]) 分数: -4.0765\n",
      "候选项: Hi I am a little([17250, 314, 716, 257, 1310]) 分数: -4.1127\n",
      "\n",
      "选择的 beam:\n",
      "beam: Hi I am a big([17250, 314, 716, 257, 1263]) 分数: -3.8471\n",
      "beam: Hi I am a very([17250, 314, 716, 257, 845]) 分数: -4.0765\n",
      "beam: Hi I am a little([17250, 314, 716, 257, 1310]) 分数: -4.1127\n",
      "\n",
      "第 2 步:\n",
      "候选项: Hi I am a big fan([17250, 314, 716, 257, 1263, 4336]) 分数: -4.2282\n",
      "候选项: Hi I am a big believer([17250, 314, 716, 257, 1263, 29546]) 分数: -7.1364\n",
      "候选项: Hi I am a big supporter([17250, 314, 716, 257, 1263, 15525]) 分数: -8.3071\n",
      "候选项: Hi I am a very good([17250, 314, 716, 257, 845, 922]) 分数: -6.7407\n",
      "候选项: Hi I am a very nice([17250, 314, 716, 257, 845, 3621]) 分数: -7.1981\n",
      "候选项: Hi I am a very happy([17250, 314, 716, 257, 845, 3772]) 分数: -7.3773\n",
      "候选项: Hi I am a little bit([17250, 314, 716, 257, 1310, 1643]) 分数: -6.2787\n",
      "候选项: Hi I am a little confused([17250, 314, 716, 257, 1310, 10416]) 分数: -7.0488\n",
      "候选项: Hi I am a little disappointed([17250, 314, 716, 257, 1310, 11679]) 分数: -7.2740\n",
      "\n",
      "选择的 beam:\n",
      "beam: Hi I am a big fan([17250, 314, 716, 257, 1263, 4336]) 分数: -4.2282\n",
      "beam: Hi I am a little bit([17250, 314, 716, 257, 1310, 1643]) 分数: -6.2787\n",
      "beam: Hi I am a very good([17250, 314, 716, 257, 845, 922]) 分数: -6.7407\n",
      "\n",
      "第 3 步:\n",
      "候选项: Hi I am a big fan of([17250, 314, 716, 257, 1263, 4336, 286]) 分数: -4.3084\n",
      "候选项: Hi I am a big fan and([17250, 314, 716, 257, 1263, 4336, 290]) 分数: -8.1861\n",
      "候选项: Hi I am a big fan.([17250, 314, 716, 257, 1263, 4336, 13]) 分数: -8.3988\n",
      "候选项: Hi I am a little bit of([17250, 314, 716, 257, 1310, 1643, 286]) 分数: -8.6324\n",
      "候选项: Hi I am a little bit worried([17250, 314, 716, 257, 1310, 1643, 7960]) 分数: -9.4857\n",
      "候选项: Hi I am a little bit older([17250, 314, 716, 257, 1310, 1643, 4697]) 分数: -9.5333\n",
      "候选项: Hi I am a very good person([17250, 314, 716, 257, 845, 922, 1048]) 分数: -9.3997\n",
      "候选项: Hi I am a very good friend([17250, 314, 716, 257, 845, 922, 1545]) 分数: -9.8804\n",
      "候选项: Hi I am a very good student([17250, 314, 716, 257, 845, 922, 3710]) 分数: -10.3733\n",
      "\n",
      "选择的 beam:\n",
      "beam: Hi I am a big fan of([17250, 314, 716, 257, 1263, 4336, 286]) 分数: -4.3084\n",
      "beam: Hi I am a big fan and([17250, 314, 716, 257, 1263, 4336, 290]) 分数: -8.1861\n",
      "beam: Hi I am a big fan.([17250, 314, 716, 257, 1263, 4336, 13]) 分数: -8.3988\n"
     ]
    }
   ],
   "source": [
    "show_beam_search_steps(model, tokenizer, prefixes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
