{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basics\n",
    "\n",
    "- [torch官方](https://github.com/pytorch/examples/tree/main/distributed/ddp-tutorial-series)\n",
    "\n",
    "\n",
    "1. multiple GPUs in a single machine/server/node: 单机多卡\n",
    "\n",
    "    - 分布式数据并行时，模型(model parameters)/ 优化器(optimizer states)每张卡都会拷贝一份(replicas)\n",
    "        - DDP始终在卡间维持着模型参数和优化器状态的同步一致性在整个训练过程中\n",
    "    \n",
    "    - DataParallel, batch input通过DistributedSampler split分发到不同的GPUs上\n",
    "        - 模型/optimizer相同，但因为输入数据不同，loss不同，反向传播时计算的梯度也不同\n",
    "        - 此时DDP如何保证卡间的同步一致性\n",
    "            - ring all-reduce algorithm\n",
    "\n",
    "\n",
    "2. ring all-reduce algorithm\n",
    "\n",
    "    - 将所有的gpus连成一个ring\n",
    "    - 同步过程，不需要等待所有的卡都计算完一轮梯度\n",
    "    - 经过这个同步过程，所有的卡的model/optimizer都会保持一致状态\n",
    "\n",
    "\n",
    "## 1.1 Ring All-Reduce Algorithm\n",
    "\n",
    "1. 简介\n",
    "    - 李沐：参数服务器\n",
    "    - 计算和同步的几个过程\n",
    "        - GPUs分别计算损失（forward），梯度（backward）\n",
    "        - 梯度聚合\n",
    "        - 模型/优化器参数的更新以及广播\n",
    "\n",
    "2. HPC的基础算法\n",
    "\n",
    "3. Ring环形拓扑结构\n",
    "    - 环形，逻辑的\n",
    "    - 两个过程：\n",
    "        - scatter-reduce\n",
    "        - all gather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DDP相关概念\n",
    "\n",
    "## 2.1 node，rank，world_size\n",
    "\n",
    "1. world，world_size\n",
    "    - world: 一个组group，包含了一组分布式训练的进程\n",
    "        - 通常，每一个gpu代表一个进程（process）\n",
    "        - world内的process可以彼此通信，所以有DDP分布式训练\n",
    "\n",
    "2. rank\n",
    "    - rank：进程的唯一标识，进程级别的概念，识别进程，因为进程之间需要通信\n",
    "    - local rank：多机多卡概念\n",
    "\n",
    "3. node\n",
    "    - 理解为一个server机器，2个servers就是2个nodes\n",
    "\n",
    "> 例如2个servers（node），各有四张卡gpus\n",
    "> world_size:2*4==8, ranks:[0,1,2,3,4,5,6,7], local_rank:[0,1,2,3], [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
