{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basics\n",
    "\n",
    "- [torch官方](https://github.com/pytorch/examples/tree/main/distributed/ddp-tutorial-series)\n",
    "\n",
    "\n",
    "1. multiple GPUs in a single machine/server/node: 单机多卡\n",
    "\n",
    "    - 分布式数据并行时，模型(model parameters)/ 优化器(optimizer states)每张卡都会拷贝一份(replicas)\n",
    "        - DDP始终在卡间维持着模型参数和优化器状态的同步一致性在整个训练过程中\n",
    "    \n",
    "    - DataParallel, batch input通过DistributedSampler split分发到不同的GPUs上\n",
    "        - 模型/optimizer相同，但因为输入数据不同，loss不同，反向传播时计算的梯度也不同\n",
    "        - 此时DDP如何保证卡间的同步一致性\n",
    "            - ring all-reduce algorithm\n",
    "\n",
    "\n",
    "2. ring all-reduce algorithm\n",
    "\n",
    "    - 将所有的gpus连成一个ring\n",
    "    - 同步过程，不需要等待所有的卡都计算完一轮梯度\n",
    "    - 经过这个同步过程，所有的卡的model/optimizer都会保持一致状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
